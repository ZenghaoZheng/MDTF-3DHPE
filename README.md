# ğŸŒ€ Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation (Published on Image and Vision Computing)


Official PyTorch implementation of our paper:
**"Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation"**
ğŸ“„ *Zenghao Zheng, Lianping Yang, Jinshan Pan, Hegui Zhu*   

[[Paper]](https://doi.org/10.48550/arXiv.2505.20611) ï½œ [[Project Page]](https://github.com/ZenghaoZheng/MDTF-3DHPE) ï½œ[[Video]](https://youtu.be/ujZxNtj1NXc)

---

## ğŸŒŸ Highlights

* âš™ï¸ **Mamba-Driven Topology Fusion (MDTF)** â€” a novel framework that integrates human topology priors into the Mamba sequence model.
* ğŸ¦´ **Bone Aware Module (BAM)** â€” provides global topological guidance in spherical coordinates.
* ğŸ” **GCN-Enhanced Vision Mamba (GEM)** â€” enhances Mamba with bidirectional GCN to capture local spatial dependencies.
* ğŸ§© **Bone-Joint Fusion Embedding** â€” fuses joint and bone information from different coordinate systems.
* ğŸ•’ **Spatiotemporal Refinement Module** â€” refines fused features for accurate 3D pose regression.
* ğŸš€ Achieves **40.0 mm MPJPE on Human3.6M** with only **1/9** the computational cost of MixSTE.

---

## Installation

```bash
git clone https://github.com/ZenghaoZheng/MDTF-3DHPE.git
cd MDTF-3DHPE
conda create -n mdtf python=3.8
conda activate mdtf
pip install -r requirements.txt
```

---

## ğŸ“¦ Dataset Preparation

### 1. Human3.6M

Download the fine-tuned Stacked Hourglass detections of [MotionBERT](https://github.com/Walter0807/MotionBERT/blob/main/docs/pose3d.md)â€™s preprocessed Human3.6M data:

ğŸ”— [Download Here](https://1drv.ms/u/s!AvAdh0LSjEOlgU7BuUZcyafu8kzc?e=vobkjZ)

Unzip it to:

```
data/motion3d
```

Then, slice the motion clips:

```bash
cd data/preprocess
python h36m.py --n-frames 243
python h36m.py --n-frames 81
python h36m.py --n-frames 27
```

---

## ğŸ§© Framework Overview

<p align="center">
  <img src="figure/train_process.jpg" width="850">
</p>

* **Stage 1:** Train the Bone Aware Module to infer bone vector orientations (spherical coordinates).
* **Stage 2:** Fuse jointâ€“bone features via Bone-Joint Embedding and refine them with Spatiotemporal Refinement Module.

---
## ğŸ§  Training

Our training process is divided into **two stages**, corresponding to the **Bone Aware Module (Stage 1)** and the **Fusion and Refinement Network (Stage 2)**.

### ğŸ©» Stage 1 â€“ Train Bone Aware Module

```bash
python train_firststage.py \
  --config configs/h36m/first.yaml \
  --new-checkpoint checkpoint/stage1 \
  --use-wandb --wandb-name boneawaremodule_stage1
```

### ğŸ§© Stage 2 â€“ Train Bone-Joint Fusion Embedding & Spatiotemporal Refinement Module

**Tiny model:**

```bash
python train_twostage.py \
  --config configs/h36m/tiny.yaml \
  --new-checkpoint checkpoint/stage2/tiny \
  --use-wandb --wandb-name stage2_tiny
```

**Large model:**

```bash
python train_twostage.py \
  --config configs/h36m/large.yaml \
  --new-checkpoint checkpoint/stage2/large \
  --use-wandb --wandb-name stage2_large
```

**Resume training:**

```bash
python train_twostage.py \
  --config configs/h36m/tiny.yaml \
  --resume \
  --checkpoint checkpoint/stage2/tiny/ \
  --new-checkpoint checkpoint/stage2/tiny/
```

---
## ğŸ’¾ Pretrained Weights

You can download pretrained weights for quick evaluation or fine-tuning.

| Stage            | Model             | Download Link                                                                                            | Save To                    |
| ---------------- | ----------------- | -------------------------------------------------------------------------------------------------------- | -------------------------- |
| Stage 1         | Bone Aware Module | [Google Drive ğŸ”—](https://drive.google.com/file/d/17CFDOW2JFeVfTLwF0BbTZuV5kW0qUV7b/view?usp=drive_link) | `checkpoint/stage1/`       |
| Stage 2 (Tiny) | MDTF-Tiny Model   | [Google Drive ğŸ”—](https://drive.google.com/file/d/1x9-45205xWoQaVB95_WRxS3k2Mr7IDlG/view?usp=sharing)    | `checkpoint/stage2/tiny/`  |
| Stage 2 (Large) | MDTF-Large Model  | [Google Drive ğŸ”—](https://drive.google.com/file/d/18mRUdE4B5Pc0KB321tltKLMWZEksNWl3/view?usp=sharing)    | `checkpoint/stage2/large/` |

---
## ğŸ§ª Evaluation

Evaluate pretrained checkpoints on **Human3.6M**:

```bash
python train.py \
  --eval-only \
  --checkpoint checkpoint/stage2/tiny \
  --checkpoint-file best_epoch.pth.tr \
  --config configs/h36m/tiny.yaml
```

---

## ğŸ¬ Demo

Our demo builds upon the visualization pipeline from [MHFormer](https://github.com/Vegetebird/MHFormer).

### 1. Download dependencies

* YOLOv3 and HRNet pretrained models from [here](https://drive.google.com/drive/folders/1_ENAMOsPM7FXmdYRbkwbFHgzQq_B_NQA?usp=sharing)
  â†’ Place in `./demo/lib/checkpoint`
* Prepare the pre-trained checkpoint of the MDTF model according to Evaluation

### 2. Place your videos

```
./demo/video/sample_video.mp4
```

### 3. Run the demo

```bash
python demo/vis.py --video sample_video.mp4
```

<p align="center">
  <img src="figure/demo.gif" width="600">
</p>

---

## ğŸ“Š Performance

| Dataset      | Model      | MPJPE â†“     | Params | MACs/frame | P-MPJPE â†“   |
| ------------ | ---------- | ----------- | ------ | ---------- | ----------- |
| Human3.6M    | Ours-Tiny  | **41.7 mm** | 0.9 M  | 12.8 M     | 34.8 mm     |
| Human3.6M    | Ours-Large | **40.0 mm** | 4.4 M  | 58.7 M     | **33.5 mm** |
| MPI-INF-3DHP | Ours-Large | **15.3 mm** | 4.4 M  | 58.7 M     | â€”           |

> âœ¨ *Our MDTF framework achieves a 0.9 mm improvement over PoseMamba and reduces computation by 9Ã— compared to MixSTE.*


<!---
## ğŸ“š Citation

If you find this work useful, please cite our paper:

```bibtex
@article{zheng2025mdtf,
  title={Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation},
  author={Zheng, Zenghao and Yang, Lianping and Pan, Jinshan and Zhu, Hegui},
  journal={Image and Vision Computing},
  year={2025}
}
```
-->
---

## ğŸ¤ Acknowledgements

We thank the authors of [MotionBERT](https://github.com/Walter0807/MotionBERT) and [MHFormer](https://github.com/Vegetebird/MHFormer) for their open-source contributions that inspired this project.

---
